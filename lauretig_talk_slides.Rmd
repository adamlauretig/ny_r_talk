---
title: "Bayesian Factorization Machines with Stan and R"
author: "Adam M. Lauretig"
date: "12/18/2019"
output: beamer_presentation
mainfont: Palatino
header-includes:
  - \usepackage{palatino}
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```


## Welcome
 
- Talk: Factorization machines:
  - What are they?
  - How do we fit them?
- Stan code $+$ simulations!
  - Start with simple model
  - Next, hierarchical model
- Finally, real data analysis:
  - Plotting model results

## Who I Am

- Senior Data Scientist, JUST Capital
- Ph.D. in Political Science (The Ohio State University); B.A. in Political Science (Grinnell College)
- Word Embeddings, Causal Inference, Bayesian Statistics, Discrete Choice Models
 
## Why We Model

- Descriptive Inference: is there an association between $\pmb{X}$ and $\pmb{y}$?
- Prediction: with $\pmb{X}$ and $\pmb{y}$, what is $\pmb{y}$ for a new $\pmb{X}$?
- Causal Inference: does changing $\pmb{X}$, change $\pmb{y}$?
- Shameless plug: Book Chapter https://www.dropbox.com/s/cmwd15wmad4coy9/Statistics%20and%20International%20Security.pdf

## Latent Variable Models/Unsupervised models

- Learning parameters to reconstruct observed data
- Ex: Principal Components Analysis, Factor Analysis, Matrix Factorization, Ideal Point Models, Word2vec
- Data $\pmb{X}_{N \times J}$ is decomposed into two low rank matrices: $\pmb{\gamma}_{N \times K}$ and $\pmb{\delta}_{K \times J}$
- Various assumptions about the structure of $\pmb{\gamma}$ and $\pmb{\delta}$


## Factorization Machines

- Combine regression with a latent variable model on the residuals
- Can plug and play with any generalized linear model

# But Why?

## Factorization Machines



- Regression Model, for one observation
- Categorical predictors $\pmb{x}_{n \in N}$, $\pmb{x}_{j \in J}$
- Outcome $\pmb{y}$ 
- Parameters $\pmb{\beta}$
- Error $\varepsilon_{nj}$
$$ y_{nj} = \pmb{x}_{n} \beta_{1} + \pmb{x}_{j} \beta_{2} + \varepsilon_{nj} $$

```{r, chunk_1, eval = FALSE, echo = TRUE, size = 'tiny'}
m1 <- lm(y ~ factor(group_1) + factor(group_2))
```


- With Interactions

$$ y_{nj} = \pmb{x}_{n} \beta_{1} + \pmb{x}_{j} \beta_{2} + \pmb{x}_{n} \times  \pmb{x}_{j} \beta_{3} + \varepsilon_{nj} $$

```{r, chunk_2, eval = FALSE, echo = TRUE, size = 'tiny'}
m1 <- lm(y ~ factor(group_1) + factor(group_2) + factor(group_1) * factor(group_2))
```

## Factorization Machines

**Problems:**

- We can only estimate $\beta_{3}$ for observed interactions
- As $N$ and $J$ grow, $\beta_{3}$ increases with size $N*J$


## Factorization Machines

Solution!

- Replace $\beta_{3}$ with the dot product of low-rank latent factors: 
- $\pmb{\gamma}_{N \times K}$
- $\pmb{\delta}_{J \times K}$

- $\beta_{3}$ is now $\gamma_{n} \cdot \delta_{j}^{\top}$

## Factorization Machines

- Interaction model:
$$ y_{nj} = \pmb{x}_{n} \beta_{1} + \pmb{x}_{j} \beta_{2} + \pmb{\gamma}_{n} \cdot \pmb{\delta_{j}}^{\top} + \varepsilon_{nj} $$

## Factorization Machines

Basic model
- Each element of $\delta_{j}$ and $\gamma_{n}$ is distributed standard normal: $\mathcal{N}(0, 1)$


\begin{align*}
y_{nj} &\sim \mathcal{N}( \pmb{x}_{n} \beta_{1} + \pmb{x}_{j} \beta_{2} +\pmb{\gamma}_{n} \cdot \pmb{\delta}_{j}^{\top}, \sigma_{y}) \\
\pmb{\beta} &\sim \mathcal{N}(0, \sigma_{\beta}) \\
\gamma_{n,k} &\sim \mathcal{N}(0,1) \\
\delta_{j,k} &\sim \mathcal{N}(0, 1) \\
\end{align*}


## Simulating Data: 

First, the regression component:

```{r, simulate_regression, echo = TRUE, eval = FALSE, size = "tiny"}
seed_to_use = 123
set.seed(seed_to_use)
# number of levels for first covariate
N <- 100
group_1 <- paste0("i", 1:N)
# number of levels for second covariate
J <- 20
group_2 <- paste0("j", 1:J)
# number of latent dimensions
K <- 5

# observed data ----
predictors <- expand.grid(group_1 = group_1, group_2 = group_2)
X_mat <- sparse.model.matrix(~ factor(group1) + factor(group_2) - 1, data = predictors)

# for sparsity, since here, we're assuming we have only dummies
# creating numeric values for each individual FE
predictors_as_numeric <- cbind(
  as.numeric(factor(predictors[, 1])), as.numeric(factor(predictors[, 2])))

# the regression part of the equation
betas <- matrix(rnorm(n = ncol(X_mat), 0, 2))
linear_predictor <- X_mat %*% betas

```

## Simulate a Factorization Machine:

Next, latent factors:
```{r simulate_fm2, echo = TRUE, eval = FALSE, size = "tiny"}

gamma_omega <- rlkjcorr(n = 1, K = K, eta = gamma_omega_prior)
delta_omega <- rlkjcorr(n = 1, K = K, eta = delta_omega_prior)

gammas <- mvrnorm(n = N, mu = rep(0, K), Sigma = gamma_omega)

# group 2 factors are deltas
deltas <- mvrnorm(n = J, mu = rep(0, K), Sigma = delta_omega)

factor_terms <-matrix(
  NA, nrow = nrow(linear_predictor), ncol = 1)

for (i in 1:nrow(predictors)) {
  g1 <- as.character(predictors[i, 1])
  g1 <- as.numeric(substr(g1, 2, nchar(g1)))
  
  g2 <- as.character(predictors[i, 2])
  g2 <- as.numeric(substr(g2, 2, nchar(g2)))
  
  factor_terms[i,] <- matrix(gammas[g1,], nrow = 1) %*%
    matrix(deltas[g2,], ncol = 1)
}

y <- linear_predictor + factor_terms + rnorm(n = nrow(linear_predictor), 0, y_sigma)

```

  

## Factorization Machines in Stan

Use Stan to fit FMs:

 - Data:

```{stan output.var= "stan_fm1_1", echo = TRUE, eval = FALSE, size = "tiny"}
data{
  int<lower = 0> N ; // number of group 1 observations
  int<lower = 0> J ; // number of group 2 observations
  int<lower = 0> K ; // number of latent dimensions
  int X[(N*J), 2]  ; // covariate matrix
  vector[(N*J)] y ; // outcome
  real<lower = 0> beta_sigma ; // sd on regression coefficients
  real<lower = 0> y_sigma ; // sd on the outcome, y
}
```

## Factorization Machines in Stan

Use Stan to fit FMs:

 - Parameters:


```{stan output.var= "stan_fm1_2", echo = TRUE, eval = FALSE, size = "tiny"}
parameters{
  vector[N] group_1_betas; // non-interacted coefficients
  vector[J] group_2_betas; // non-interacted coefficients
  matrix[N, K] gammas; // individual factors
  matrix[J, K] deltas; // group 2 factors
}

transformed parameters{
  real linear_predictor[(N*J)] ;
  for(i in 1:(N*J)){
    linear_predictor[i] = 
      group_1_betas[X[i, 1]] + group_2_betas[X[i, 2]] + 
      (gammas[X[i, 1], ]  * deltas[ X[i, 2], ]');
  }
}
```


## Factorization Machines in Stan

Use Stan to fit FMs:

 - Model:
```{stan output.var= "stan_fm1_3", echo = TRUE, eval = FALSE, size = "tiny"}
model{
  
  // regression coefficients
  group_1_betas ~ normal(0, beta_sigma) ;
  group_2_betas ~ normal(0, beta_sigma) ;

  // latent factors
  for(n in 1:N){
    gammas[n, ] ~ normal(rep_vector(0, K), 1) ;
  }
  
  for(j in 1:J){
    deltas[j, ] ~ normal(rep_vector(0, K), 1) ;
  }
  
  // outcome
  y ~ normal(linear_predictor, y_sigma) ;
}
```

## Factorization Machines in Stan

Use Stan to fit FMs:

- Model Checking
```{stan output.var= "stan_fm1_4", echo = TRUE, eval = FALSE, size = "tiny"}

generated quantities{
  real y_pred[(N*J)]  ;
  y_pred = normal_rng(linear_predictor, y_sigma) ;
}
```


## Fitting a Model to Simulated Data

- Using `simulate_data` in `simulation_function.R`.
- Two versions:
- 100 members of group 1, 20 members of group 2
- 20 members of group 1, 100 members of group 2
- Fit model in `stan`, using `NUTS`

## Fitting a Model to Simulated Data

```{r, stan_fm1_5, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.height= 4, dpi = 640, cache = TRUE }
library(rstan)
library(bayesplot)
library(cowplot)
load("simulated_data.rdata")
load("simple_fm.rdata")
color_scheme_set("red")

m1_posterior <- extract(m1_fit)
p1 <- ppc_dens_overlay(y = fm_simulation_1[[1]]$y, yrep = m1_posterior$y_pred[seq(1, 4000, 10), ]) + labs(title = "Model 1 Posterior Predictive Check")
m2_posterior <- extract(m2_fit)
p2 <- ppc_dens_overlay(y = fm_simulation_2[[1]]$y, yrep = m2_posterior$y_pred[seq(1, 4000, 10), ]) + labs(title = "Model 2 Posterior Predictive Check")

plot_grid(p1, p2, align = "h")


```


## Fitting a Model to Simulated Data


```{r, stan_fm1_6, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.height= 4, dpi = 640, cache=TRUE}
library(rstan)
library(ggplot2)
load("simulated_data.rdata")
load("simple_fm.rdata")
m1_posterior <- extract(m1_fit)
m2_posterior <- extract(m2_fit)

mse <- function(y_pred, y_actual){
  mean((y_actual - y_pred)^2)
}

m1_mse <- apply(X = m1_posterior$y_pred, MARGIN = 1, FUN = mse, y_actual = fm_simulation_1[[1]]$y)
m2_mse <- apply(X = m2_posterior$y_pred, MARGIN = 1, FUN = mse, y_actual = fm_simulation_2[[1]]$y)
combined <- rbind(data.frame(varname = "Model 1 Mean Squared Error", mse = m1_mse),
data.frame(varname = "Model 2 Mean Squared Error", mse = m2_mse))

ggplot(data = combined, aes(mse)) + geom_density() + 
  facet_wrap(~varname) + 
  theme_minimal() + theme(panel.grid = element_blank(), axis.text.y = element_blank()) + 
  labs(title = "", y = "", x = "Distribution of Mean-Squared Error Over Posterior Distibution")
# TODO: make this pretty
```


# Now, Let's Get Hierarchical!

## Hierarchical Factorization Machines

- Basic FM implementation assumes all parameters are independent
- Hierarchical: Sharing parameters across groups
- Sharing parameters = sharing information!


## Hierarchical Factorization Machines

\begin{align*}
y_{nj} &\sim \mathcal{N}( \pmb{x}_{n} \beta_{1} + \pmb{x}_{j} \beta_{2} + \pmb{\gamma}_{n} \cdot \pmb{\delta}_{j}^{\top}, \sigma_{y}) \\
\pmb{\beta} &\sim \mathcal{N}(0, \sigma_{\beta}) \\
\gamma_{n,} &\sim \mathcal{MVN}(\pmb{\mu}_{\pmb{\gamma}}, \pmb{\Sigma}_{\pmb{\gamma}}) \\
\delta_{j,} &\sim \mathcal{MVN}(\pmb{\mu}_{\pmb{\delta}}, \pmb{\Sigma}_{\pmb{\delta}}) \\
{\mu}_{\pmb{\gamma}} &\sim \mathcal{N}(0, 1) \\
{\mu}_{\pmb{\delta}} &\sim \mathcal{N}(0, 1) \\
\pmb{\Sigma}_{\pmb{\gamma}} &= \pmb{\sigma}_{\pmb{\gamma}} \pmb{\Omega}_{\pmb{\gamma}} \\
\pmb{\Sigma}_{\pmb{\delta}} &= \pmb{\sigma}_{\pmb{\delta}} \pmb{\Omega}_{\pmb{\delta}} \\
\pmb{\Omega}_{\pmb{\gamma}} &\sim \mathcal{LKJ}(\Omega_{0}) \\
\pmb{\Omega}_{\pmb{\delta}} &\sim \mathcal{LKJ}(\Omega_{0}) \\
\pmb{\sigma}_{\pmb{\gamma}} &\sim \mathcal{TN}(0, \sigma_{0}) \\
\pmb{\sigma}_{\pmb{\delta}} &\sim \mathcal{TN}(0, \sigma_{0}) \\
\end{align*}

## Hierarchical Factorization Machines in Stan

- Data

```{stan output.var= "stan_fm2_1", echo = TRUE, eval = FALSE, size = "tiny"}
data{
  int<lower = 0> N ; // number of group 1 observations
  int<lower = 0> J ; // number of group 2 observations
  int<lower = 0> K ; // number of latent dimensions
  int X[(N*J), 2]  ; // covariate matrix
  vector[(N*J)] y ; // outcome
  real<lower = 0> beta_sigma ; // sd on regression coefficients
  real<lower = 0> y_sigma ; // sd on the outcome, y
  real<lower = 0> gamma_sigma_prior ; //sd on gamma factors
  real<lower = 0> delta_sigma_prior ; //sd on dela factors
  real<lower = 0> gamma_omega_prior ; // prior on omega value for gamma
  real<lower = 0> delta_omega_prior ; // prior on omega value for delta

}
```

## Hierarchical Factorization Machines in Stan

- Parameters

```{stan output.var= "stan_fm2_2", echo = TRUE, eval = FALSE, size = "tiny"}
parameters{
  vector[N] group_1_betas; // non-interacted coefficients
  vector[J] group_2_betas; // non-interacted coefficients
  
  matrix[N, K] gamma_mu ; //gamma prior mean
  vector<lower=0>[K] gamma_sigma ; // embedding SD
  cholesky_factor_corr[K] gamma_omega; // correlation matrix
  matrix[K, N] gamma_a ; // for non-centered parameterization
  
  matrix[J , K] delta_mu ; //delta prior mean
  vector<lower=0>[K] delta_sigma ; // embedding SD
  cholesky_factor_corr[K] delta_omega; // correlation matrix
  matrix[K, J] delta_a ; // for non-centered parameterization
}
```


## Hierarchical Factorization Machines in Stan

- Transformed Parameters

```{stan output.var= "stan_fm2_2_1", echo = TRUE, eval = FALSE, size = "tiny"}

transformed parameters{
  real linear_predictor[(N*J)] ;
  matrix[N, K] gammas ;
  matrix[J, K] deltas ;

  gammas = gamma_mu + (diag_pre_multiply(gamma_sigma, gamma_omega) * gamma_a)' ;
  deltas = delta_mu + (diag_pre_multiply(delta_sigma, delta_omega) * delta_a)' ;

  for(i in 1:(N*J)){
    linear_predictor[i] = 
      group_1_betas[X[i, 1]] + group_2_betas[X[i, 2]] + 
      (gammas[X[i, 1], ]  * deltas[ X[i, 2], ]');
  }
}
```


## Hierarchical Factorization Machines in Stan

- Model

```{stan output.var= "stan_fm2_3", echo = TRUE, eval = FALSE, size = "tiny"}
model{
  
  // regression coefficients
  group_1_betas ~ normal(0, beta_sigma) ;
  group_2_betas ~ normal(0, beta_sigma) ;

  // factor sd: half-normal distribution
  gamma_sigma ~ normal(0, gamma_sigma_prior) ; 
  delta_sigma ~ normal(0, delta_sigma_prior) ; 

  // correlation matrices
  gamma_omega ~ lkj_corr_cholesky(gamma_omega_prior) ;
  delta_omega ~ lkj_corr_cholesky(delta_omega_prior) ;
  
  // for non-centered parameterization
  to_vector(gamma_a) ~ std_normal() ;
  to_vector(delta_a) ~ std_normal() ;
  
  
  // hierarchical means
  for(n in 1:N){
    gamma_mu[n, ] ~ normal(0, 1) ;
  }  
  for(j in 1:J){
    delta_mu[j, ] ~ normal(0, 1) ;
  }
  
  // outcome
  y ~ normal(linear_predictor, y_sigma) ;
}
```

## Fitting a (Hierarchical) Model to Simulated Data

```{r, stan_fm2_4, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.height= 4, dpi = 640, cache = TRUE }
library(rstan)
library(bayesplot)
library(cowplot)
load("simulated_data.rdata")
load("hierarchical_fm.rdata")
color_scheme_set("red")

m1_posterior <- extract(m1_fit)
p1 <- ppc_dens_overlay(y = fm_simulation_1[[1]]$y, yrep = m1_posterior$y_pred[seq(1, 4000, 10), ]) + labs(title = "Hierarchical Model 1 Posterior Predictive")
m2_posterior <- extract(m2_fit)
p2 <- ppc_dens_overlay(y = fm_simulation_2[[1]]$y, yrep = m2_posterior$y_pred[seq(1, 4000, 10), ]) + labs(title = "Hierarchical Model 2 Posterior Predictive")

plot_grid(p1, p2, align = "h")


```

## Fitting a (Hierarchical) Model to Simulated Data

```{r, stan_fm2_5, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, fig.height= 4, dpi = 640, cache=TRUE}
library(rstan)
library(ggplot2)
load("simulated_data.rdata")
load("hierarchical_fm.rdata")
m1_posterior <- extract(m1_fit)
m2_posterior <- extract(m2_fit)

mse <- function(y_pred, y_actual){
  mean((y_actual - y_pred)^2)
}

m1_mse <- apply(X = m1_posterior$y_pred, MARGIN = 1, FUN = mse, y_actual = fm_simulation_1[[1]]$y)
m2_mse <- apply(X = m2_posterior$y_pred, MARGIN = 1, FUN = mse, y_actual = fm_simulation_2[[1]]$y)
combined <- rbind(data.frame(varname = "Model 1 Mean Squared Error", mse = m1_mse),
data.frame(varname = "Model 2 Mean Squared Error", mse = m2_mse))

ggplot(data = combined, aes(mse)) + geom_density()+ 
  facet_wrap(~varname) + 
  theme_minimal() + theme(panel.grid = element_blank(), axis.text.y = element_blank()) + 
  labs(title = "", y = "", x = "Distribution of Mean-Squared Error Over Posterior Distibution")

# TODO: make this pretty
```


# Fitting a Model to Real Data

## Dataset

- Fearon and Laitin, APSR 2003
- Ethnicity, Insurgency, and Civil War 
- Goal: Understand relationship between ethnic fractionalization and the onset of Civil War
- Country-year data: All countries in the world, 1946-1999
- Data and Code: https://web.stanford.edu/group/ethnic/publicdata/publicdata.html
- Fit model with logistic regression

## Factorization Machine
  
- Modify Existing FM Code:
  - Logistic Regression
  - Additional Covariates
- Note: Rare events outcome ($1\%$ positive outcomes)
  

## Model Fit

```{r, fearon_laitin_model_fit, cache = TRUE, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, dpi = 640}
library(data.table)
library(Matrix)
library(foreign)
library(rstan)
library(bayesplot)
library(ggplot2)
library(umap)
library(MASS)
load("fm_model_3_fit.rdata")
fl_civil_war <- read.dta("repdata.dta")
fl_civil_war <-fl_civil_war[ fl_civil_war$onset < 2, ]

fl_civil_war <- fl_civil_war[ !(fl_civil_war$war == 1 & fl_civil_war$onset == 0), ]
m1 <- glm(onset~warl+gdpenl+lmtnest+polity2l+ethfrac+relfrac, data = fl_civil_war, family = binomial())
m1_preds <- predict(m1, type = "response")


data_subset <- fl_civil_war[
  c("warl", "gdpenl", "lpopl1", "lmtnest", "polity2l", "ethfrac", "relfrac", 
    "ncontig",  "Oil", "nwstate", "instab",  "cname", "year", "onset")]
data_subset <- na.omit(data_subset)
data_subset <- data_subset[ data_subset$onset < 2, ]
groups <- data.frame(cname = factor(data_subset$cname), 
           cnumber = as.numeric(factor(data_subset$cname)), 
           year = factor(data_subset$year), 
           cyear = as.numeric(factor(data_subset$year)))
X <- as.matrix(groups[, c(2, 4)])
X2 <- as.matrix(data_subset[, 1:11])
y <- as.numeric(data_subset$onset)
data <- list(n_obs = nrow(X), X = X, X2 = X2, y = y, N = max(X[, 1]), 
  J = max(X[, 2]), K = 5,
     beta_sigma = 1,
     gamma_sigma_prior = 1, 
     delta_sigma_prior = 1, 
     gamma_omega_prior = 2, 
     delta_omega_prior = 2)
m2_posterior <- extract(m2_fit)
predictions <- data.frame(y = y, y_preds = colMeans(m2_posterior$y_pred), m1_preds = m1_preds)

log_loss_binary <- function(actual, predicted, eps = 1e-15) {
  predicted = pmin(pmax(predicted, eps), 1-eps)
  - (sum(actual * log(predicted) + (1 - actual) * log(1 - predicted))) / length(actual)
  }

ll_fm <- log_loss_binary(actual = predictions$y, predicted = predictions$y_preds)
ll_logit <- log_loss_binary(actual = predictions$y, predicted = predictions$m1_preds)


ggplot(data = predictions, aes(x = y, y = y_preds)) + 
  geom_jitter(width = .2, alpha = .3, size = 1) + 
  geom_jitter(data = predictions, aes(x = y, y = m1_preds), 
              width = .2, size = 1, alpha = .3, color = "red") + 
  theme_minimal() + 
  theme(
    panel.grid = element_blank(), 
    axis.text.x = element_blank(), 
    plot.title = element_text(size = 20),
    plot.subtitle = element_text(size = 16)) +
  labs(
    title = "Predicted Probabilities for Each Model;\nRed is Logistic Regression, Black is Factorization Machine", 
    subtitle = paste0(
      "Logistic Regression Log-loss: ", round(ll_logit, 3), 
      " Factorization Machine Log-loss: ", round(ll_fm, 3)), 
    y = "Mean Predicted Probabilities", x = "Outcome Categories")

```


## Clustering Factors

Use Sammon Mapping to cluster factors

```{r, sammon_mapping, cache = TRUE, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, dpi = 640}
library(data.table)
library(Matrix)
library(foreign)
library(rstan)
library(bayesplot)
library(ggplot2)
library(umap)
library(MASS)
set.seed(123)
load("fm_model_3_fit.rdata")
fl_civil_war <- read.dta("repdata.dta")
fl_civil_war <-fl_civil_war[ fl_civil_war$onset < 2, ]
fl_civil_war <- fl_civil_war[ !(fl_civil_war$war == 1 & fl_civil_war$onset == 0), ]

data_subset <- fl_civil_war[
  c("warl", "gdpenl", "lpopl1", "lmtnest", "polity2l", "ethfrac", "relfrac", 
    "ncontig",  "Oil", "nwstate", "instab",  "cname", "year", "onset")]
data_subset <- na.omit(data_subset)
data_subset <- data_subset[ data_subset$onset < 2, ]
groups <- data.frame(cname = factor(data_subset$cname), 
           cnumber = as.numeric(factor(data_subset$cname)), 
           year = factor(data_subset$year), 
           cyear = as.numeric(factor(data_subset$year)))
X <- as.matrix(groups[, c(2, 4)])
X2 <- as.matrix(data_subset[, 1:11])
y <- as.numeric(data_subset$onset)
data <- list(n_obs = nrow(X), X = X, X2 = X2, y = y, N = max(X[, 1]), 
  J = max(X[, 2]), K = 5,
     beta_sigma = 1,
     gamma_sigma_prior = 1, 
     delta_sigma_prior = 1, 
     gamma_omega_prior = 2, 
     delta_omega_prior = 2)
m2_posterior <- extract(m2_fit)
country_means <- apply(m2_posterior$gammas, c(2:3), FUN = mean)
countries <- unique(groups[, 1:2])
# countries[order(countries$cnumber),]
rownames(country_means) <- as.character(countries[order(countries$cnumber),]$cname)
country_sim <- sammon(dist(country_means), trace = FALSE)
country_sim_df <- data.frame(
  countries = rownames(country_sim$points), 
  x = country_sim$points[, 1], y = country_sim$points[, 2])

ggplot(data = country_sim_df, aes( x = x, y = y)) + 
  geom_text(aes(label = countries)) + theme_minimal()


```


## Clustering Factors

Zoom in on hairball

```{r, sammon_mapping2, cache = TRUE, echo = FALSE, message=FALSE, error=FALSE, warning=FALSE, dpi = 640}
country_sim_df2 <- country_sim_df[ country_sim_df$x > (-.05) & country_sim_df$x < .05, ]
country_sim_df2 <- country_sim_df2[ country_sim_df2$y > (-.05) & country_sim_df2$y < .05, ]
ggplot(data = country_sim_df2, aes( x = x, y = y)) + 
  geom_text(aes(label = countries)) + theme_minimal()

```

## Wrap Up

- Factorization Machines: Extend GLM models with matrix factorization on residuals
- Two versions:
  - Factors distibuted $\mathcal{N}(0, 1)$
  - Hierarchical Model
- Use dimensionality reduction to plot factors
- What if there are more than two interactions?
  - Tensor Factorization/Exponential Machines

## Thank You!

\center


Code and Slides: https://github.com/adamlauretig/ny_r_talk
Twitter: @Lauretig


\huge Thank you!

\center